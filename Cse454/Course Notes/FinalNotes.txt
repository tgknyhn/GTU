Dimensionality Reduction
    Curse of Dimensionality
        Increasing number of features is not benefitial all the time
        Even worse, adding more feature to data set may lead to worse performance
        The reason behind this, the number of samples we need to train with each dimension increases exponantially
    Objective
        Objective of the dimensionality reduction is to improve classification accuracy by choosing best set of features 
        of lower dimensionality 
    Feature Selection
        Steps
            Find the possible subsets
            Pick the optimal or near-optimal subset with respect to objective function
        Search Methods
            Exhaustive 
            Heuristic
                Naive Search
                    For the n features, we sort these features according to a objective function by their "goodness" and
                    select the best d number of features
                    Disadvantages : 
                        - Correlation between the features ignored
                        - Best pair of features, may not contain the best feature
                SFS (Sequential Forward Selection)
                    First we find the best feature according to a objective function.
                    Then we create pairs with one of the remaining features and this best feature.
                    We select the best pair. Then move on to best triplet. And so on. Until we reach the desired number of features.
                    Disadvantages : 
                        - It can't remove any feature when it become non useful with pairs of features
                SBS (Sequential Backward Selection)
                    This time first we find the worst feature in the data set by using a objective function.
                    Then we discard this worst feature and find next worst feature. We do this until we reach the desired number of features.
                    Disadvantages :
                        - It can't use discarded feature even if it becomes useful with pairs of features 
                BDS (Bi-directional Search)
                    In this heuristic search method we use both SFS and SBS methods.
            Randomized
        Evaluation Methods
            Wrapper
                Wrapper evaluation methods take into account the relationship between features, which makes it more accurate than filter methods.
                But checking for feature interactions for each subset is computationally expensive.
            Filter
                Filter evaluation methods doesn't consider interaction between the features. It uses same objective function for all subsets.
                Which makes it faster than wrapper methods. But selecting best features doesn't guarentee the best result. Because feature interactions
                might decrease a feature's 'goodness'
    Feature Extraction
        Information Loss
            PCA (Principle Component Analysis)
                Standardize the data
                    - PCA is sensitive to the scale of the data. So it is important to standardize the features by substracting the mean 
                      and scaling to unit variance
                Compute the covariance matrix
                    - We need to measure how the features vary together. So to do that, we need to compute the covariance matrix of the
                      standardized data
                Compute the eigenvectors and eigenvalues
                    - eigenvectors of covariance matrix are the directions in which the data varies most. And the eigenvalues are the 
                      corresponding magnitues of the variations
                Sort the eigenvectors by decreasing eigenvalues
                    - We sort in the decreasing order because the eigenvectors with the largest eigenvalus are to most important features.
                Choose the top k eigenvectors
                    - Select the top k eigenvectors, where k is the number of dimensions you want to reduce the data
                Transform the data
                    - Project the data onto the space spanned by the top k eigenvectors 
            Choosing the K value
                - One way to select k is to compute the cumulative sum of the eigenvalues and 
                  choose the smallest k such that the cumulative sum is greater than or equal to the desired level of retained variance. 
                  For example, if you want to retain at least 80% of the variance, 
                  you can select the smallest k such that the cumulative sum of the eigenvalues is at least 0.8.
                - Another way to select k is to plot the eigenvalues in decreasing order and 
                  choose the number of dimensions at the "elbow" of the curve, where the rate of decrease in the eigenvalues slows. 
                  This is known as the "elbow method".
        Discriminatory Information
            LDA (Linear Discriminant Analysis)
                - Compute the d-dimensional mean vectors for the different classes from the training set.
                - Compute the scatter matrices (in-between-class and within-class scatter matrix).
                - Compute the eigenvectors and corresponding eigenvalues of the scatter matrices.
                - Sort the eigenvectors by decreasing eigenvalues and choose k eigenvectors with 
                  the largest eigenvalues to form a d × k dimensional matrix W (where every column represents an eigenvector).
                - Use this d × k eigenvector matrix to transform the samples onto the new subspace. 
                  This can be summarized by the matrix multiplication: Y = X × W (where X is a n × d-dimensional matrix representing the n samples, 
                  and y are the transformed n × k-dimensional samples in the new subspace).
                - Perform classification on the transformed samples in the new subspace using a suitable classifier.
    
FastText
    is a word embedding method that uses a deep neural network to learn word vectors from a large dataset of text. 
    It is trained on a bag of n-grams (a sequence of n consecutive words) rather than individual words, 
    and is able to handle large amounts of data quickly and efficiently.
Word2Vec
    is a word embedding method that uses a two-layer neural network to learn word vectors from a large dataset of text. 
    It is trained on a continuous bag of words (CBOW) or a skip-gram model, and can handle large amounts of data quickly and efficiently.
Differences
    - Word2vec is trained on individual words, while fasttext is trained on n-grams. 
      This means that fasttext is able to capture more context and meaning from the text, as it takes into account the words surrounding a particular word.
    - Word2vec is not able to handle unknown words (words that are not in the training dataset), as it is trained on individual words. 
      Fasttext, on the other hand, is able to handle unknown words by using the n-grams surrounding the unknown word to make a prediction
Skip-gram 
    - It is a machine learning model used to learn the embeddings of words in a dataset. 
      It does this by using a neural network to predict the context (the surrounding words) of a target word based on its embedding, or vector representation. 
      The goal of the model is to learn embeddings that are meaningful and capture the relationships between words in the dataset.

Outlier Detection
    Types of outliers
        Global
            - Significantly different than the rest of the data points
        Contextual
            - Significantly different than the rest of the data points in a particular context
            - May be normal in other contexts
        Collective
            - A group of data points that are significantly different from the rest of the data
    Challenges of outlier detection
        - Definition of an outlier
        - Large datasets
        - Multiple dimensions
        - Distributed data
        - Data quality
    Outlier Detection
        Supervised Methods
        Unsupervised Methods
        Semi-supervised Methods
        Statistical Methods
        Proximity-Based Methods
        Clustering-Based Methods
        Classification-Based Methods


Ödev Slaytları